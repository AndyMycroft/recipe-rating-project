# Recipe Rating Analysis
### DSC80 Final Project – Fall 2025

Welcome to my project website! This site presents my full data science analysis on the Recipes & Ratings dataset from Food.com.

---

## Introduction
Online recipe platforms rely heavily on user ratings to determine which recipes
are promoted, recommended, or featured. However, recipes can vary widely in
their complexity, cooking time, nutritional content, and structure. This raises
an important question: what actually makes a recipe well rated?

In this project, I analyze the Recipes and Ratings dataset from Food.com, which
contains tens of thousands of user-submitted recipes along with user ratings and
interaction data. By combining recipe metadata with aggregated user ratings, this
dataset provides a rich opportunity to study how recipe characteristics relate
to user preferences.

The central question of this project is:

What factors influence how highly a recipe is rated, and can we predict whether
a recipe will receive a high average rating (≥ 4)?

Understanding these patterns is valuable for multiple audiences. Home cooks may
gain insights into how to design clearer or more appealing recipes, while recipe
platforms can use such insights to improve recommendation systems and search
rankings. More broadly, this project demonstrates how data science can be used to
connect human preferences with structural features in real-world datasets.

## Cleaning and Exploratory Data Analysis
This histogram shows the distribution of the number of ingredients across recipes.
Most recipes use between 5 and 15 ingredients, indicating that extremely simple or
extremely complex recipes are relatively uncommon.
<iframe
  src="assets/n_ingredients_distribution.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

This scatter plot shows the relationship between the number of ingredients and average recipe rating.
Most recipes contain between 5 and 15 ingredients, and average ratings are generally high across all levels of recipe complexity.
However, there is no clear linear relationship between ingredient count and rating, as recipes with both few and many ingredients can receive high or low ratings.
This suggests that ingredient count alone does not strongly determine how a recipe is rated, motivating the use of additional features and predictive models in later steps.
<iframe
  src="assets/ingredients_vs_rating.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

## Assessment of Missingness

This plot compares the distribution of number of ingredients between recipes
with missing ratings and those with observed ratings.

<iframe
  src="assets/ingredients_by_rating_missing.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

## Hypothesis Testing
The histogram above shows the permutation distribution of the test statistic, defined as the difference in mean scores between recipes with more ingredients and those with fewer ingredients. This distribution was generated by repeatedly permuting the group labels and recalculating the mean difference under the null hypothesis.
The observed mean difference falls entirely within the permutation distribution, indicating that this level of difference is common in the case of random assignment. This visualization is consistent with the p-value of the permutation test, suggesting that there is no statistically significant evidence of a difference in mean scores between recipes with more and fewer ingredients.
<iframe
  src="assets/hypothesis_permutation_distribution.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>
## Framing a Prediction Problem
We frame a binary classification problem to predict whether a recipe will receive a high average rating (4 or higher).
The response variable indicates whether a recipe is well-received by users, based on its average rating. This framing reflects a practical goal for recipe platforms: identifying recipes that are likely to be positively received.
Predictions are made at the time a recipe is submitted, using only information available at that point, such as the number of ingredients, preparation steps, cooking time, and nutritional information. User ratings and reviews are intentionally excluded to avoid data leakage.
Model performance is evaluated using accuracy, which is appropriate given the binary nature of the task and the relatively balanced classes.

## Baseline Model
As a baseline, a logistic regression classifier was trained using only basic
numerical features available at the time of recipe submission.

The features used include:
- Number of ingredients
- Number of preparation steps
- Total cooking time

This model serves as a simple and interpretable reference point rather than an
attempt to maximize performance. It achieved an accuracy of approximately
**0.90** on a held-out test set.

The baseline model establishes a benchmark against which more complex models can
be compared.


## Final Model
The final model improves upon the baseline by incorporating engineered features
that better capture recipe complexity and nutritional structure.

In addition to the baseline features, the following engineered features were
introduced:
- Calories per ingredient
- Steps per minute

A logistic regression model was again used to maintain interpretability. Model
hyperparameters were selected using cross-validation.

The final model achieved a slightly higher accuracy than the baseline model,
indicating that the engineered features help capture additional information
relevant to predicting high-rated recipes.

## Fairness Analysis
*(text + plot)*

---
